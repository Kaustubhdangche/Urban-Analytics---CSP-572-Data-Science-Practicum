{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b15c4bef-65f2-4164-935c-317accfaf691",
   "metadata": {},
   "source": [
    "### City Owned Land Filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62edbf25-86a3-4a3a-8580-d04d1c2171f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: ['ID', 'pin', 'Address', 'Managing Organization', 'Property Status', 'Date of Acquisition', 'Date of Disposition', 'Sales Status', 'Sale Offering Status', 'Sale Offering Reason', 'Sq. Ft.', 'Square Footage - City Estimate', 'Land Value (2022)', 'Ward', 'Community Area Number', 'Community Area Name', 'Zoning Classification', 'Zip Code', 'Last Update', 'Application Use', 'Grouped Parcels', 'Application Deadline', 'Offer Round', 'Application URL', 'X Coordinate', 'Y Coordinate', 'Latitude', 'Longitude', 'Location']\n",
      "Number of rows: 20579\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "file_path1 = \"C:/Users/kaur6/Downloads/Urban Analytics/City-Owned_Land_Inventory_Cleaned.csv\"\n",
    "df1 = pd.read_csv(file_path1)\n",
    "\n",
    "# Display columns and number of rows\n",
    "print(\"Columns in the dataset:\", df1.columns.tolist())\n",
    "print(\"Number of rows:\", df1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ee4479c-92ea-4caf-9b8a-18234a5508c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'Property Status': ['Owned by City' 'Sold' 'Ownd by City' 'Leased' 'Sold By City'\n",
      " 'Not City Owned' 'Sold by City' 'In Acquisition' nan]\n"
     ]
    }
   ],
   "source": [
    "# Print unique values for 'Sales Status' and 'Sale Offering Status'\n",
    "print(\"Unique values in 'Property Status':\", df1['Property Status'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "157c90c6-8652-475b-86ff-d79363a69a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to: C:/Users/kaur6/Downloads/Urban Analytics/Owned_By_City_Properties.csv\n"
     ]
    }
   ],
   "source": [
    "# Standardize the 'Property Status' column\n",
    "df1['Property Status'] = df1['Property Status'].str.strip()  # Remove leading/trailing spaces\n",
    "\n",
    "# Filter rows where 'Property Status' is 'Owned by City' or its misspelled version\n",
    "filtered_df = df1[df1['Property Status'].isin(['Owned by City', 'Ownd by City'])].copy()\n",
    "\n",
    "# Correct the misspelled value\n",
    "filtered_df['Property Status'] = 'Owned by City'\n",
    "\n",
    "# Save to a new CSV file\n",
    "output_path = \"C:/Users/kaur6/Downloads/Urban Analytics/Owned_By_City_Properties.csv\"\n",
    "filtered_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Filtered data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b185f67-22a5-47f0-bee6-afb017fd8154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 12425\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows:\", filtered_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70542a46-256e-48ce-8f5d-9695bb7f8bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null value count for each column:\n",
      " ID                                    0\n",
      "pin                                   0\n",
      "Address                             724\n",
      "Managing Organization              7380\n",
      "Property Status                       0\n",
      "Date of Acquisition                4551\n",
      "Date of Disposition               12421\n",
      "Sales Status                       3578\n",
      "Sale Offering Status              11951\n",
      "Sale Offering Reason              11950\n",
      "Sq. Ft.                             391\n",
      "Square Footage - City Estimate     8415\n",
      "Land Value (2022)                  8407\n",
      "Ward                                806\n",
      "Community Area Number               806\n",
      "Community Area Name                 809\n",
      "Zoning Classification               809\n",
      "Zip Code                            394\n",
      "Last Update                           0\n",
      "Application Use                   11909\n",
      "Grouped Parcels                   12119\n",
      "Application Deadline              11628\n",
      "Offer Round                       11622\n",
      "Application URL                   11723\n",
      "X Coordinate                        806\n",
      "Y Coordinate                        806\n",
      "Latitude                            806\n",
      "Longitude                           806\n",
      "Location                            806\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "file = pd.read_csv(\"C:/Users/kaur6/Downloads/Urban Analytics/Owned_By_City_Properties.csv\")\n",
    "# Count null values for each column\n",
    "null_counts = file.isnull().sum()\n",
    "# Print null value counts\n",
    "print(\"Null value count for each column:\\n\", null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88837096-7493-4af5-9dcd-88287ed0bafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where 'Sales Status' is 'Application Closed': 625\n",
      "Number of rows where 'Sales Status' is 'Offered': 1003\n"
     ]
    }
   ],
   "source": [
    "# Count rows where 'Sales Status' is 'Application Closed'\n",
    "application_closed_count = (file['Sales Status'] == 'Application Closed').sum()\n",
    "# Print the count\n",
    "print(\"Number of rows where 'Sales Status' is 'Application Closed':\", application_closed_count)\n",
    "# Count rows where 'Sales Status' is 'Offered'\n",
    "offered_count = (file['Sales Status'] == 'Offered').sum()\n",
    "# Print the count\n",
    "print(\"Number of rows where 'Sales Status' is 'Offered':\", offered_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3472515-e0dc-4bf4-a68a-32b484814798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with 'Application Closed' and 'Offered' removed. Cleaned data saved to: C:/Users/kaur6/Downloads/Urban Analytics/Cleaned_City_Owned_Land.csv\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows where 'Sales Status' is 'Application Closed' or 'Offered'\n",
    "df_cleaned = file[~file['Sales Status'].isin(['Application Closed', 'Offered'])]\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "output_path = \"C:/Users/kaur6/Downloads/Urban Analytics/Cleaned_City_Owned_Land.csv\"\n",
    "df_cleaned.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Rows with 'Application Closed' and 'Offered' removed. Cleaned data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c072558-659d-4565-a55a-709370d80830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 10797\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows:\", df_cleaned.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a941df63-799e-4fb8-b9ce-8fdbfee16a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file with 'pin' column saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "input_file = \"C:/Users/kaur6/Downloads/Urban Analytics/Cleaned_City_Owned_Land.csv\"\n",
    "output_file = \"C:/Users/kaur6/Downloads/Urban Analytics/Pin_Only.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Extract only the 'pin' column\n",
    "df_pin = df[['pin']]\n",
    "\n",
    "# Save to a new CSV file\n",
    "df_pin.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"CSV file with 'pin' column saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d9d94-cd8c-4d33-aab0-d36bb6b14a43",
   "metadata": {},
   "source": [
    "### Cook County Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f0b0f8e-9245-4551-b893-3db5e8171328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: ['pin', 'tax_year', 'card_num', 'class', 'township_code', 'proration_key_pin', 'pin_proration_rate', 'card_proration_rate', 'cdu', 'pin_is_multicard', 'pin_num_cards', 'pin_is_multiland', 'pin_num_landlines', 'year_built', 'building_sqft', 'land_sqft', 'num_bedrooms', 'num_rooms', 'num_full_baths', 'num_half_baths', 'num_fireplaces', 'type_of_residence', 'construction_quality', 'num_apartments', 'attic_finish', 'garage_attached', 'garage_area_included', 'garage_size', 'garage_ext_wall_material', 'attic_type', 'basement_type', 'ext_wall_material', 'central_heating', 'repair_condition', 'basement_finish', 'roof_material', 'single_v_multi_family', 'site_desirability', 'num_commercial_units', 'renovation', 'recent_renovation', 'porch', 'central_air', 'design_plan']\n"
     ]
    }
   ],
   "source": [
    "# File path\n",
    "file_path3 = \"C:/Users/kaur6/Downloads/Urban Analytics/final_cook_county_data_with_same_dtype.csv\"\n",
    "\n",
    "# Read the first few rows to get column names (without loading the entire dataset)\n",
    "df_sample = pd.read_csv(file_path3, nrows=5)  # Reads only the first 5 rows\n",
    "print(\"Columns in the dataset:\", df_sample.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be66ddc-63e8-4a3c-93fe-03d0a9591f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 1 from Cook County dataset...\n",
      "Processed chunk 2 from Cook County dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaur6\\AppData\\Local\\Temp\\ipykernel_14440\\2773846171.py:19: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path3, chunksize=chunk_size):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 3 from Cook County dataset...\n",
      "Processed chunk 4 from Cook County dataset...\n",
      "Processed chunk 5 from Cook County dataset...\n",
      "Processed chunk 6 from Cook County dataset...\n",
      "Processed chunk 7 from Cook County dataset...\n",
      "Processed chunk 8 from Cook County dataset...\n",
      "Processed chunk 9 from Cook County dataset...\n",
      "Processed chunk 10 from Cook County dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaur6\\AppData\\Local\\Temp\\ipykernel_14440\\2773846171.py:19: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path3, chunksize=chunk_size):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 11 from Cook County dataset...\n",
      "Processed chunk 12 from Cook County dataset...\n",
      "Processed chunk 13 from Cook County dataset...\n",
      "Processed chunk 14 from Cook County dataset...\n",
      "Processed chunk 15 from Cook County dataset...\n",
      "Processed chunk 16 from Cook County dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaur6\\AppData\\Local\\Temp\\ipykernel_14440\\2773846171.py:19: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path3, chunksize=chunk_size):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 17 from Cook County dataset...\n",
      "Processed chunk 18 from Cook County dataset...\n",
      "Processed chunk 19 from Cook County dataset...\n",
      "Processed chunk 20 from Cook County dataset...\n",
      "Processed chunk 21 from Cook County dataset...\n",
      "Processed chunk 22 from Cook County dataset...\n",
      "Processed chunk 23 from Cook County dataset...\n",
      "Processed chunk 24 from Cook County dataset...\n",
      "Processed chunk 25 from Cook County dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaur6\\AppData\\Local\\Temp\\ipykernel_14440\\2773846171.py:19: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(file_path3, chunksize=chunk_size):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 26 from Cook County dataset...\n",
      "Processed chunk 27 from Cook County dataset...\n",
      "Processed chunk 28 from Cook County dataset...\n",
      "Processed chunk 29 from Cook County dataset...\n",
      "Processed chunk 1 from Pin_Only dataset...\n",
      "Cook County dataset with appended pins saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "file_path3 = \"C:/Users/kaur6/Downloads/Urban Analytics/final_cook_county_data_with_same_dtype.csv\"\n",
    "pin_file = \"C:/Users/kaur6/Downloads/Urban Analytics/Pin_Only.csv\"\n",
    "output_file = \"C:/Users/kaur6/Downloads/Urban Analytics/final_cook_county_appended_vacant_lands.csv\"\n",
    "\n",
    "# Define chunk size (adjust based on memory availability)\n",
    "chunk_size = 1000000  # Process 500K rows at a time\n",
    "\n",
    "# Read column names first (to ensure consistency)\n",
    "df_sample = pd.read_csv(file_path3, nrows=5)  # Read first few rows\n",
    "columns = df_sample.columns  # Extract column names\n",
    "\n",
    "# Write the header first before processing\n",
    "with open(output_file, 'w', newline='') as f_out:\n",
    "    df_sample.iloc[:0].to_csv(f_out, index=False)  # Write only the header\n",
    "\n",
    "# Process Cook County dataset in chunks\n",
    "chunk_num = 0\n",
    "for chunk in pd.read_csv(file_path3, chunksize=chunk_size):\n",
    "    chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "    chunk_num += 1\n",
    "    print(f\"Processed chunk {chunk_num} from Cook County dataset...\")\n",
    "\n",
    "# Process Pin_Only dataset in chunks\n",
    "chunk_num = 0\n",
    "for pin_chunk in pd.read_csv(pin_file, chunksize=chunk_size):\n",
    "    # Create an empty DataFrame with the same columns\n",
    "    pin_chunk_expanded = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    # Assign 'pin' values while keeping other columns blank\n",
    "    pin_chunk_expanded['pin'] = pin_chunk['pin']\n",
    "\n",
    "    # Append to output file\n",
    "    pin_chunk_expanded.to_csv(output_file, index=False, mode='a', header=False)\n",
    "    \n",
    "    chunk_num += 1\n",
    "    print(f\"Processed chunk {chunk_num} from Pin_Only dataset...\")\n",
    "\n",
    "print(\"Cook County dataset with appended pins saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8a626a7-0984-4848-91e4-7187da8972f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: ['pin', 'tax_year', 'card_num', 'class', 'township_code', 'proration_key_pin', 'pin_proration_rate', 'card_proration_rate', 'cdu', 'pin_is_multicard', 'pin_num_cards', 'pin_is_multiland', 'pin_num_landlines', 'year_built', 'building_sqft', 'land_sqft', 'num_bedrooms', 'num_rooms', 'num_full_baths', 'num_half_baths', 'num_fireplaces', 'type_of_residence', 'construction_quality', 'num_apartments', 'attic_finish', 'garage_attached', 'garage_area_included', 'garage_size', 'garage_ext_wall_material', 'attic_type', 'basement_type', 'ext_wall_material', 'central_heating', 'repair_condition', 'basement_finish', 'roof_material', 'single_v_multi_family', 'site_desirability', 'num_commercial_units', 'renovation', 'recent_renovation', 'porch', 'central_air', 'design_plan']\n",
      "Total number of rows: 28450035\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# Path to the merged dataset\n",
    "merged_file_path = \"C:/Users/kaur6/Downloads/Urban Analytics/final_cook_county_data_with_same_dtype.csv\"\n",
    "\n",
    "### **Step 1: Get Column Names Without Loading Full Data**\n",
    "with open(merged_file_path, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    columns = next(reader)  # Read only the first row (column names)\n",
    "\n",
    "print(\"Columns in the dataset:\", columns)\n",
    "\n",
    "### **Step 2: Count Total Rows Efficiently**\n",
    "row_count = sum(1 for _ in open(merged_file_path, 'r')) - 1  # Subtract 1 for the header\n",
    "print(\"Total number of rows:\", row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9230cf1a-433a-4730-8638-4eb478c08fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: ['pin', 'tax_year', 'card_num', 'class', 'township_code', 'proration_key_pin', 'pin_proration_rate', 'card_proration_rate', 'cdu', 'pin_is_multicard', 'pin_num_cards', 'pin_is_multiland', 'pin_num_landlines', 'year_built', 'building_sqft', 'land_sqft', 'num_bedrooms', 'num_rooms', 'num_full_baths', 'num_half_baths', 'num_fireplaces', 'type_of_residence', 'construction_quality', 'num_apartments', 'attic_finish', 'garage_attached', 'garage_area_included', 'garage_size', 'garage_ext_wall_material', 'attic_type', 'basement_type', 'ext_wall_material', 'central_heating', 'repair_condition', 'basement_finish', 'roof_material', 'single_v_multi_family', 'site_desirability', 'num_commercial_units', 'renovation', 'recent_renovation', 'porch', 'central_air', 'design_plan']\n",
      "Total number of rows: 28460832\n"
     ]
    }
   ],
   "source": [
    "# Path to the merged dataset\n",
    "merged_file_path = \"C:/Users/kaur6/Downloads/Urban Analytics/final_cook_county_appended_vacant_lands.csv\"\n",
    "\n",
    "### **Step 1: Get Column Names Without Loading Full Data**\n",
    "with open(merged_file_path, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    columns = next(reader)  # Read only the first row (column names)\n",
    "\n",
    "print(\"Columns in the dataset:\", columns)\n",
    "\n",
    "### **Step 2: Count Total Rows Efficiently**\n",
    "row_count = sum(1 for _ in open(merged_file_path, 'r')) - 1  # Subtract 1 for the header\n",
    "print(\"Total number of rows:\", row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597ef014-dfb4-4b65-8c9f-da121f55c495",
   "metadata": {},
   "source": [
    "### Parcel_CC.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4472fd9-beda-4426-bd92-647055ce5d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: ['name', 'pin10', 'job_no', 'longitude', 'latitude', 'census_tract', 'tract_geoid', 'tract_white_perc', 'tract_black_perc', 'inclusion_source']\n",
      "Number of rows: 1905645\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the CSV file\n",
    "file_path = \"C:/Users/kaur6/Downloads/Urban Analytics/Parcel_CC.csv\"\n",
    "df = pd.read_csv(file_path, dtype={'name': str, 'inclusion_source': str}, low_memory=False)\n",
    "\n",
    "# Display columns and number of rows\n",
    "print(\"Columns in the dataset:\", df.columns.tolist())\n",
    "print(\"Number of rows:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82a161d-7bd4-4df9-af2b-ce242d71819d",
   "metadata": {},
   "source": [
    "### Matching Parcel_CC.cv and Cook County merged with Vacant Land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9feb0ab-eeaf-4274-b51d-ff9f85903109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 1...\n",
      "Processed chunk 2...\n",
      "Processed chunk 3...\n",
      "Processed chunk 4...\n",
      "Processed chunk 5...\n",
      "Processed chunk 6...\n",
      "Processed chunk 7...\n",
      "Processed chunk 8...\n",
      "Processed chunk 9...\n",
      "Processed chunk 10...\n",
      "Processed chunk 11...\n",
      "Processed chunk 12...\n",
      "Processed chunk 13...\n",
      "Processed chunk 14...\n",
      "Processed chunk 15...\n",
      "Processed chunk 16...\n",
      "Processed chunk 17...\n",
      "Processed chunk 18...\n",
      "Processed chunk 19...\n",
      "Processed chunk 20...\n",
      "Processed chunk 21...\n",
      "Processed chunk 22...\n",
      "Processed chunk 23...\n",
      "Processed chunk 24...\n",
      "Processed chunk 25...\n",
      "Processed chunk 26...\n",
      "Processed chunk 27...\n",
      "Processed chunk 28...\n",
      "Processed chunk 29...\n",
      "Matching unique pins saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "file1 = \"C:/Users/kaur6/Downloads/Urban Analytics/final_cook_county_appended_vacant_lands.csv\"\n",
    "file2 = \"C:/Users/kaur6/Downloads/Urban Analytics/Parcel_CC.csv\"\n",
    "output_file = \"C:/Users/kaur6/Downloads/Urban Analytics/matching_unique_pins.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 1000000  # Process 1000K rows at a time\n",
    "\n",
    "# Load second dataset's 'name' column as strings into a set for fast lookup\n",
    "parcel_pins = set(pd.read_csv(file2, usecols=['name'], dtype=str)['name'])\n",
    "\n",
    "# Open output file and write header\n",
    "with open(output_file, 'w', newline='') as f_out:\n",
    "    f_out.write(\"pin\\n\")  # Writing the header manually\n",
    "\n",
    "# Track unique matched pins\n",
    "unique_matched_pins = set()\n",
    "\n",
    "# Process first dataset in chunks\n",
    "chunk_num = 0\n",
    "for chunk in pd.read_csv(file1, usecols=['pin'], dtype=str, chunksize=chunk_size):\n",
    "    # Keep only matching pins (comparing as strings)\n",
    "    matching_pins = chunk[chunk['pin'].isin(parcel_pins)]\n",
    "\n",
    "    # Remove duplicates within the chunk and check global uniqueness\n",
    "    new_unique_pins = set(matching_pins['pin']) - unique_matched_pins  # Find truly new pins\n",
    "    unique_matched_pins.update(new_unique_pins)  # Add new ones to the tracker\n",
    "\n",
    "    # Convert to DataFrame and write to file\n",
    "    pd.DataFrame({'pin': list(new_unique_pins)}).to_csv(output_file, index=False, mode='a', header=False)\n",
    "\n",
    "    chunk_num += 1\n",
    "    print(f\"Processed chunk {chunk_num}...\")\n",
    "\n",
    "print(\"Matching unique pins saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f62cd621-8e43-4d5c-a8ac-f42b69277f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 1...\n",
      "Processed chunk 2...\n",
      "Processed chunk 3...\n",
      "Processed chunk 4...\n",
      "Processed chunk 5...\n",
      "Processed chunk 6...\n",
      "Processed chunk 7...\n",
      "Processed chunk 8...\n",
      "Processed chunk 9...\n",
      "Processed chunk 10...\n",
      "Processed chunk 11...\n",
      "Processed chunk 12...\n",
      "Processed chunk 13...\n",
      "Processed chunk 14...\n",
      "Processed chunk 15...\n",
      "Processed chunk 16...\n",
      "Processed chunk 17...\n",
      "Processed chunk 18...\n",
      "Processed chunk 19...\n",
      "Processed chunk 20...\n",
      "Processed chunk 21...\n",
      "Processed chunk 22...\n",
      "Processed chunk 23...\n",
      "Processed chunk 24...\n",
      "Processed chunk 25...\n",
      "Processed chunk 26...\n",
      "Processed chunk 27...\n",
      "Processed chunk 28...\n",
      "Processed chunk 29...\n",
      "Final dataset with inclusion_source saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "file1 = \"C:/Users/kaur6/Downloads/Urban Analytics/final_cook_county_appended_vacant_lands.csv\"  # Big dataset\n",
    "file2 = \"C:/Users/kaur6/Downloads/Urban Analytics/Parcel_CC.csv\"  # Smaller dataset\n",
    "output_file = \"C:/Users/kaur6/Downloads/Urban Analytics/final_dataset_with_inclusion_source.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 1000000  # Process 500K rows at a time\n",
    "\n",
    "# Load smaller dataset's 'name' and 'inclusion_source' columns into a dictionary for fast lookup\n",
    "parcel_data = pd.read_csv(file2, usecols=['name', 'inclusion_source'], dtype=str)\n",
    "parcel_dict = dict(zip(parcel_data['name'], parcel_data['inclusion_source']))  # Convert to dictionary\n",
    "\n",
    "# Open output file and write header first\n",
    "with open(output_file, 'w', newline='') as f_out:\n",
    "    df_sample = pd.read_csv(file1, nrows=5)  # Read few rows to get column names\n",
    "    df_sample['inclusion_source'] = None  # Add new column\n",
    "    df_sample.iloc[:0].to_csv(f_out, index=False)  # Write only header\n",
    "\n",
    "# Process big dataset in chunks\n",
    "chunk_num = 0\n",
    "for chunk in pd.read_csv(file1, dtype=str, chunksize=chunk_size):\n",
    "    # Add 'inclusion_source' column by mapping 'pin' to values in the dictionary\n",
    "    chunk['inclusion_source'] = chunk['pin'].map(parcel_dict)\n",
    "\n",
    "    # Append the updated chunk to the output file\n",
    "    chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "\n",
    "    chunk_num += 1\n",
    "    print(f\"Processed chunk {chunk_num}...\")\n",
    "\n",
    "print(\"Final dataset with inclusion_source saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e3ab479-2ed3-4ca7-95d1-9465f7d27f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 1...\n",
      "Processed chunk 2...\n",
      "Processed chunk 3...\n",
      "Processed chunk 4...\n",
      "Processed chunk 5...\n",
      "Processed chunk 6...\n",
      "Processed chunk 7...\n",
      "Processed chunk 8...\n",
      "Processed chunk 9...\n",
      "Processed chunk 10...\n",
      "Processed chunk 11...\n",
      "Processed chunk 12...\n",
      "Processed chunk 13...\n",
      "Processed chunk 14...\n",
      "Processed chunk 15...\n",
      "Processed chunk 16...\n",
      "Processed chunk 17...\n",
      "Processed chunk 18...\n",
      "Processed chunk 19...\n",
      "Processed chunk 20...\n",
      "Processed chunk 21...\n",
      "Processed chunk 22...\n",
      "Processed chunk 23...\n",
      "Processed chunk 24...\n",
      "Processed chunk 25...\n",
      "Processed chunk 26...\n",
      "Processed chunk 27...\n",
      "Processed chunk 28...\n",
      "Processed chunk 29...\n",
      "\n",
      "Conversion completed! File saved as: C:/Users/kaur6/Downloads/Urban Analytics/final_dataset_inclusion_string.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# File paths\n",
    "file_path = \"C:/Users/kaur6/Downloads/Urban Analytics/final_dataset_with_inclusion_source.csv\"\n",
    "output_file = \"C:/Users/kaur6/Downloads/Urban Analytics/final_dataset_inclusion_string.csv\"\n",
    "\n",
    "# Chunk size\n",
    "chunk_size = 1000000  \n",
    "\n",
    "# Read first chunk to get column names\n",
    "df_sample = pd.read_csv(file_path, nrows=5)\n",
    "df_sample['inclusion_source'] = df_sample['inclusion_source'].astype(str)  # Convert to string\n",
    "df_sample.iloc[:0].to_csv(output_file, index=False)  # Write only headers\n",
    "\n",
    "# Process in chunks\n",
    "chunk_num = 0\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size, low_memory=False):\n",
    "    # Convert to string and handle NaNs\n",
    "    chunk['inclusion_source'] = chunk['inclusion_source'].fillna(\"Unknown\").astype(str)\n",
    "    \n",
    "    # Save chunk to file\n",
    "    chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "    \n",
    "    chunk_num += 1\n",
    "    print(f\"Processed chunk {chunk_num}...\")\n",
    "\n",
    "print(\"\\nConversion completed! File saved as:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a3a4a-690d-46ff-9655-041708b43a82",
   "metadata": {},
   "source": [
    "### Cook County Data with Inclusion Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb5a5be-8d79-48ec-ac1b-30e142831bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: ['pin', 'tax_year', 'card_num', 'class', 'township_code', 'proration_key_pin', 'pin_proration_rate', 'card_proration_rate', 'cdu', 'pin_is_multicard', 'pin_num_cards', 'pin_is_multiland', 'pin_num_landlines', 'year_built', 'building_sqft', 'land_sqft', 'num_bedrooms', 'num_rooms', 'num_full_baths', 'num_half_baths', 'num_fireplaces', 'type_of_residence', 'construction_quality', 'num_apartments', 'attic_finish', 'garage_attached', 'garage_area_included', 'garage_size', 'garage_ext_wall_material', 'attic_type', 'basement_type', 'ext_wall_material', 'central_heating', 'repair_condition', 'basement_finish', 'roof_material', 'single_v_multi_family', 'site_desirability', 'num_commercial_units', 'renovation', 'recent_renovation', 'porch', 'central_air', 'design_plan', 'inclusion_source']\n",
      "\n",
      "Sample rows:\n",
      "               pin  tax_year  card_num  class  township_code  \\\n",
      "0  31074070140000      2021         1  '295'             32   \n",
      "1  31074070140000      2022         1  '295'             32   \n",
      "2  31074070140000      2023         1  '295'             32   \n",
      "3  31074070140000      2024         1  '295'             32   \n",
      "4  31074070160000      2003         1  '295'             32   \n",
      "\n",
      "   proration_key_pin  pin_proration_rate  card_proration_rate cdu  \\\n",
      "0                NaN                   1                    0  AV   \n",
      "1                NaN                   1                    0  AV   \n",
      "2                NaN                   1                    0  AV   \n",
      "3                NaN                   1                    0  AV   \n",
      "4                NaN                   1                    0  AV   \n",
      "\n",
      "   pin_is_multicard  ...      roof_material single_v_multi_family  \\\n",
      "0             False  ...  Shingle + Asphalt         Single-Family   \n",
      "1             False  ...  Shingle + Asphalt         Single-Family   \n",
      "2             False  ...  Shingle + Asphalt         Single-Family   \n",
      "3             False  ...  Shingle + Asphalt         Single-Family   \n",
      "4              True  ...  Shingle + Asphalt         Single-Family   \n",
      "\n",
      "       site_desirability  num_commercial_units  renovation  recent_renovation  \\\n",
      "0  Not Relevant To Value                     0          No              False   \n",
      "1  Not Relevant To Value                     0          No              False   \n",
      "2  Not Relevant To Value                     0          No              False   \n",
      "3  Not Relevant To Value                     0          No              False   \n",
      "4  Not Relevant To Value                     0          No              False   \n",
      "\n",
      "     porch  central_air  design_plan  inclusion_source  \n",
      "0  Unknown  Central A/C   Stock Plan           Unknown  \n",
      "1  Unknown  Central A/C   Stock Plan           Unknown  \n",
      "2  Unknown  Central A/C   Stock Plan           Unknown  \n",
      "3  Unknown  Central A/C   Stock Plan           Unknown  \n",
      "4  Unknown  Central A/C   Stock Plan           Unknown  \n",
      "\n",
      "[5 rows x 45 columns]\n",
      "\n",
      "Column Data Types:\n",
      " pin                           int64\n",
      "tax_year                      int64\n",
      "card_num                      int64\n",
      "class                        object\n",
      "township_code                 int64\n",
      "proration_key_pin           float64\n",
      "pin_proration_rate            int64\n",
      "card_proration_rate           int64\n",
      "cdu                          object\n",
      "pin_is_multicard               bool\n",
      "pin_num_cards                 int64\n",
      "pin_is_multiland             object\n",
      "pin_num_landlines             int64\n",
      "year_built                    int64\n",
      "building_sqft                 int64\n",
      "land_sqft                     int64\n",
      "num_bedrooms                  int64\n",
      "num_rooms                     int64\n",
      "num_full_baths                int64\n",
      "num_half_baths                int64\n",
      "num_fireplaces                int64\n",
      "type_of_residence            object\n",
      "construction_quality         object\n",
      "num_apartments                int64\n",
      "attic_finish                 object\n",
      "garage_attached              object\n",
      "garage_area_included         object\n",
      "garage_size                  object\n",
      "garage_ext_wall_material     object\n",
      "attic_type                   object\n",
      "basement_type                object\n",
      "ext_wall_material            object\n",
      "central_heating              object\n",
      "repair_condition             object\n",
      "basement_finish              object\n",
      "roof_material                object\n",
      "single_v_multi_family        object\n",
      "site_desirability            object\n",
      "num_commercial_units          int64\n",
      "renovation                   object\n",
      "recent_renovation              bool\n",
      "porch                        object\n",
      "central_air                  object\n",
      "design_plan                  object\n",
      "inclusion_source             object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# File path\n",
    "file_path = \"C:/Users/kaur6/Downloads/Urban Analytics/final_dataset_inclusion_string.csv\"\n",
    "\n",
    "# Load only a small sample to check structure\n",
    "df_sample = pd.read_csv(file_path, dtype={'pin_is_multiland': str}, low_memory=False, nrows=5)\n",
    "\n",
    "# Print column names and first few rows\n",
    "print(\"Columns in the dataset:\", df_sample.columns.tolist())\n",
    "print(\"\\nSample rows:\\n\", df_sample.head())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nColumn Data Types:\\n\", df_sample.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5da1d4cc-67db-4fd9-8d76-acaed3bcf141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of rows: 28460832\n"
     ]
    }
   ],
   "source": [
    "# Count total rows efficiently\n",
    "total_rows = sum(1 for _ in open(file_path)) - 1  # Subtract 1 for header\n",
    "print(f\"\\nTotal number of rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "238585e9-8950-4b73-8bb9-db2943fb4837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in 'pin': 1140302\n",
      "\n",
      "Unique values in 'inclusion_source': 4\n"
     ]
    }
   ],
   "source": [
    "# Load only necessary columns in chunks\n",
    "key_columns = ['pin', 'inclusion_source']  # Adjust based on dataset\n",
    "unique_counts = {}\n",
    "\n",
    "# Process in chunks to count unique values\n",
    "chunk_size = 1000000\n",
    "for chunk in pd.read_csv(file_path, usecols=key_columns, dtype=str, chunksize=chunk_size):\n",
    "    for col in key_columns:\n",
    "        unique_counts[col] = unique_counts.get(col, set()).union(set(chunk[col].dropna()))\n",
    "\n",
    "# Print unique counts\n",
    "for col, unique_vals in unique_counts.items():\n",
    "    print(f\"\\nUnique values in '{col}': {len(unique_vals)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0063b12d-e967-486d-a316-8ef42be5ce20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Unique Values and Their Counts in 'inclusion_source':\n",
      "  Unknown: 19549501\n",
      "  cdot bus routes: 4435827\n",
      "  gtfs rail stop points: 4406293\n",
      "  osm rail entrance, exit, station: 69211\n",
      "\n",
      "NaN Count in 'inclusion_source': 0\n",
      "\n",
      "Completed processing 'inclusion_source' unique value counts!\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store counts\n",
    "inclusion_counts = {}\n",
    "\n",
    "# Chunk size\n",
    "chunk_size = 1000000  \n",
    "\n",
    "# Process in chunks\n",
    "for chunk in pd.read_csv(file_path, usecols=['inclusion_source'], dtype=str, chunksize=chunk_size, low_memory=False):\n",
    "    # Drop NaNs and count occurrences\n",
    "    value_counts = chunk['inclusion_source'].value_counts(dropna=False).to_dict()\n",
    "    \n",
    "    # Aggregate counts\n",
    "    for key, count in value_counts.items():\n",
    "        inclusion_counts[key] = inclusion_counts.get(key, 0) + count\n",
    "\n",
    "# Print unique values with counts\n",
    "print(\"\\n🔹 Unique Values and Their Counts in 'inclusion_source':\")\n",
    "for value, count in sorted(inclusion_counts.items(), key=lambda x: -x[1]):  # Sort by count (descending)\n",
    "    print(f\"  {value}: {count}\")\n",
    "\n",
    "# Print NaN count separately\n",
    "nan_count = inclusion_counts.get('nan', 0) + inclusion_counts.get(None, 0)  # Handle 'nan' and None\n",
    "print(f\"\\nNaN Count in 'inclusion_source': {nan_count}\")\n",
    "print(\"\\nCompleted processing 'inclusion_source' unique value counts!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fcb34e2-3454-45d8-946f-12b9c4e6358f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 First 10-20 Rows where 'inclusion_source' is NOT 'Unknown':\n",
      "               pin  tax_year  card_num  class  township_code  \\\n",
      "0   10254070010000      1999         1  '203'             75   \n",
      "1   10254070010000      2000         1  '203'             75   \n",
      "2   10254070010000      2001         1  '203'             75   \n",
      "3   10254070010000      2002         1  '203'             75   \n",
      "4   10254070010000      2003         1  '203'             75   \n",
      "5   10254070010000      2004         1  '203'             75   \n",
      "6   10254070010000      2005         1  '203'             75   \n",
      "7   10254070010000      2006         1  '203'             75   \n",
      "8   10254070010000      2007         1  '203'             75   \n",
      "9   10254070010000      2008         1  '203'             75   \n",
      "10  10254070010000      2009         1  '204'             75   \n",
      "11  10254070010000      2010         1  '204'             75   \n",
      "12  10254070010000      2011         1  '204'             75   \n",
      "13  10254070010000      2012         1  '204'             75   \n",
      "14  10254070010000      2013         1  '204'             75   \n",
      "15  10254070010000      2014         1  '204'             75   \n",
      "16  10254070010000      2015         1  '204'             75   \n",
      "17  10254070010000      2016         1  '204'             75   \n",
      "18  10254070010000      2017         1  '204'             75   \n",
      "19  10254070010000      2018         1  '204'             75   \n",
      "\n",
      "    proration_key_pin  pin_proration_rate  card_proration_rate cdu  \\\n",
      "0                 NaN                   1                    0  AV   \n",
      "1                 NaN                   1                    0  AV   \n",
      "2                 NaN                   1                    0  AV   \n",
      "3                 NaN                   1                    0  AV   \n",
      "4                 NaN                   1                    0  AV   \n",
      "5                 NaN                   1                    0  AV   \n",
      "6                 NaN                   1                    0  AV   \n",
      "7                 NaN                   1                    0  AV   \n",
      "8                 NaN                   1                    0  AV   \n",
      "9                 NaN                   1                    0  AV   \n",
      "10                NaN                   1                    0  AV   \n",
      "11                NaN                   1                    0  AV   \n",
      "12                NaN                   1                    0  AV   \n",
      "13                NaN                   1                    0  AV   \n",
      "14                NaN                   1                    0  AV   \n",
      "15                NaN                   1                    0  AV   \n",
      "16                NaN                   1                    0  AV   \n",
      "17                NaN                   1                    0  AV   \n",
      "18                NaN                   1                    0  AV   \n",
      "19                NaN                   1                    0  AV   \n",
      "\n",
      "    pin_is_multicard  ...      roof_material  single_v_multi_family  \\\n",
      "0              False  ...  Shingle + Asphalt          Single-Family   \n",
      "1              False  ...  Shingle + Asphalt          Single-Family   \n",
      "2              False  ...  Shingle + Asphalt          Single-Family   \n",
      "3              False  ...  Shingle + Asphalt          Single-Family   \n",
      "4              False  ...  Shingle + Asphalt          Single-Family   \n",
      "5              False  ...  Shingle + Asphalt          Single-Family   \n",
      "6              False  ...  Shingle + Asphalt          Single-Family   \n",
      "7              False  ...  Shingle + Asphalt          Single-Family   \n",
      "8              False  ...  Shingle + Asphalt          Single-Family   \n",
      "9              False  ...  Shingle + Asphalt          Single-Family   \n",
      "10             False  ...  Shingle + Asphalt          Single-Family   \n",
      "11             False  ...  Shingle + Asphalt          Single-Family   \n",
      "12             False  ...  Shingle + Asphalt          Single-Family   \n",
      "13             False  ...  Shingle + Asphalt          Single-Family   \n",
      "14             False  ...  Shingle + Asphalt          Single-Family   \n",
      "15             False  ...  Shingle + Asphalt          Single-Family   \n",
      "16             False  ...  Shingle + Asphalt          Single-Family   \n",
      "17             False  ...  Shingle + Asphalt          Single-Family   \n",
      "18             False  ...  Shingle + Asphalt          Single-Family   \n",
      "19             False  ...  Shingle + Asphalt          Single-Family   \n",
      "\n",
      "        site_desirability  num_commercial_units  renovation  \\\n",
      "0     Beneficial To Value                     0          No   \n",
      "1     Beneficial To Value                     0          No   \n",
      "2     Beneficial To Value                     0          No   \n",
      "3     Beneficial To Value                     0          No   \n",
      "4     Beneficial To Value                     0          No   \n",
      "5     Beneficial To Value                     0          No   \n",
      "6     Beneficial To Value                     0          No   \n",
      "7   Not Relevant To Value                     0          No   \n",
      "8   Not Relevant To Value                     0          No   \n",
      "9   Not Relevant To Value                     0          No   \n",
      "10  Not Relevant To Value                     0          No   \n",
      "11  Not Relevant To Value                     0          No   \n",
      "12  Not Relevant To Value                     0          No   \n",
      "13  Not Relevant To Value                     0          No   \n",
      "14  Not Relevant To Value                     0          No   \n",
      "15  Not Relevant To Value                     0          No   \n",
      "16  Not Relevant To Value                     0          No   \n",
      "17  Not Relevant To Value                     0          No   \n",
      "18  Not Relevant To Value                     0          No   \n",
      "19  Not Relevant To Value                     0          No   \n",
      "\n",
      "    recent_renovation           porch     central_air  design_plan  \\\n",
      "0               False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "1               False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "2               False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "3               False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "4               False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "5               False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "6               False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "7               False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "8               False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "9               False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "10              False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "11              False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "12              False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "13              False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "14              False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "15              False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "16              False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "17              False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "18              False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "19              False  Frame Enclosed  No Central A/C   Stock Plan   \n",
      "\n",
      "    inclusion_source  \n",
      "0    cdot bus routes  \n",
      "1    cdot bus routes  \n",
      "2    cdot bus routes  \n",
      "3    cdot bus routes  \n",
      "4    cdot bus routes  \n",
      "5    cdot bus routes  \n",
      "6    cdot bus routes  \n",
      "7    cdot bus routes  \n",
      "8    cdot bus routes  \n",
      "9    cdot bus routes  \n",
      "10   cdot bus routes  \n",
      "11   cdot bus routes  \n",
      "12   cdot bus routes  \n",
      "13   cdot bus routes  \n",
      "14   cdot bus routes  \n",
      "15   cdot bus routes  \n",
      "16   cdot bus routes  \n",
      "17   cdot bus routes  \n",
      "18   cdot bus routes  \n",
      "19   cdot bus routes  \n",
      "\n",
      "[20 rows x 45 columns]\n",
      "\n",
      "✅ Done! Stopped after finding the first valid rows.\n"
     ]
    }
   ],
   "source": [
    "# Store valid rows\n",
    "valid_rows = []\n",
    "\n",
    "# Process in chunks\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size, low_memory=False):\n",
    "    # Filter rows where 'inclusion_source' is NOT 'Unknown'\n",
    "    filtered_chunk = chunk[chunk['inclusion_source'] != \"Unknown\"]\n",
    "    \n",
    "    # Collect valid rows (entire rows, not just 'inclusion_source')\n",
    "    valid_rows.extend(filtered_chunk.head(20).values.tolist())  # Convert to list for efficiency\n",
    "    \n",
    "    # Stop if we have 10-20 rows\n",
    "    if len(valid_rows) >= 10:\n",
    "        break\n",
    "\n",
    "# Convert back to DataFrame\n",
    "result_df = pd.DataFrame(valid_rows, columns=filtered_chunk.columns)\n",
    "\n",
    "# Print the first 10-20 valid rows (entire rows)\n",
    "print(\"\\n🔹 First 10-20 Rows where 'inclusion_source' is NOT 'Unknown':\")\n",
    "print(result_df)\n",
    "\n",
    "print(\"\\n✅ Done! Stopped after finding the first valid rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6812fc2-5318-44e6-95f4-badcd49ff3cd",
   "metadata": {},
   "source": [
    "### Checking cook county dataset for inclusion score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c3ef8-6d66-41c7-98f2-c8646555241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# File paths\n",
    "file1 = \"C:/Users/kaur6/Downloads/Urban Analytics/final_cook_county_data_with_same_dtype.csv\"  # Big dataset\n",
    "file2 = \"C:/Users/kaur6/Downloads/Urban Analytics/Parcel_CC.csv\"  # Smaller dataset\n",
    "output_file = \"C:/Users/kaur6/Downloads/Urban Analytics/final_cook_county_data_with_inclusion_source.csv\"\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 1000000  # Process 500K rows at a time\n",
    "\n",
    "# Load smaller dataset's 'name' and 'inclusion_source' columns into a dictionary for fast lookup\n",
    "parcel_data = pd.read_csv(file2, usecols=['name', 'inclusion_source'], dtype=str)\n",
    "parcel_dict = dict(zip(parcel_data['name'], parcel_data['inclusion_source']))  # Convert to dictionary\n",
    "\n",
    "# Open output file and write header first\n",
    "with open(output_file, 'w', newline='') as f_out:\n",
    "    df_sample = pd.read_csv(file1, nrows=5)  # Read few rows to get column names\n",
    "    df_sample['inclusion_source'] = None  # Add new column\n",
    "    df_sample.iloc[:0].to_csv(f_out, index=False)  # Write only header\n",
    "\n",
    "# Process big dataset in chunks\n",
    "chunk_num = 0\n",
    "for chunk in pd.read_csv(file1, dtype=str, chunksize=chunk_size):\n",
    "    # Add 'inclusion_source' column by mapping 'pin' to values in the dictionary\n",
    "    chunk['inclusion_source'] = chunk['pin'].map(parcel_dict)\n",
    "\n",
    "    # Append the updated chunk to the output file\n",
    "    chunk.to_csv(output_file, index=False, mode='a', header=False)\n",
    "\n",
    "    chunk_num += 1\n",
    "    print(f\"Processed chunk {chunk_num}...\")\n",
    "\n",
    "print(\"Final dataset with inclusion_source saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26e99c40-6ab1-41ae-85b1-693f599c6b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Unique Values and Their Counts in 'inclusion_source':\n",
      "  Unknown: 19549501\n",
      "  cdot bus routes: 4435827\n",
      "  gtfs rail stop points: 4406293\n",
      "  osm rail entrance, exit, station: 69211\n",
      "\n",
      "NaN Count in 'inclusion_source': 0\n",
      "\n",
      "Completed processing 'inclusion_source' unique value counts!\n"
     ]
    }
   ],
   "source": [
    "# File path\n",
    "file_path = \"C:/Users/kaur6/Downloads/Urban Analytics/final_dataset_inclusion_string.csv\"\n",
    "# Dictionary to store counts\n",
    "inclusion_counts = {}\n",
    "\n",
    "# Chunk size\n",
    "chunk_size = 1000000  \n",
    "\n",
    "# Process in chunks\n",
    "for chunk in pd.read_csv(file_path, usecols=['inclusion_source'], dtype=str, chunksize=chunk_size, low_memory=False):\n",
    "    # Drop NaNs and count occurrences\n",
    "    value_counts = chunk['inclusion_source'].value_counts(dropna=False).to_dict()\n",
    "    \n",
    "    # Aggregate counts\n",
    "    for key, count in value_counts.items():\n",
    "        inclusion_counts[key] = inclusion_counts.get(key, 0) + count\n",
    "\n",
    "# Print unique values with counts\n",
    "print(\"\\n🔹 Unique Values and Their Counts in 'inclusion_source':\")\n",
    "for value, count in sorted(inclusion_counts.items(), key=lambda x: -x[1]):  # Sort by count (descending)\n",
    "    print(f\"  {value}: {count}\")\n",
    "\n",
    "# Print NaN count separately\n",
    "nan_count = inclusion_counts.get('nan', 0) + inclusion_counts.get(None, 0)  # Handle 'nan' and None\n",
    "print(f\"\\nNaN Count in 'inclusion_source': {nan_count}\")\n",
    "print(\"\\nCompleted processing 'inclusion_source' unique value counts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f09b8f-b843-430a-93fb-67ab74afd6c2",
   "metadata": {},
   "source": [
    "#### Means all the vacant lands are not found in the parcel_cc.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e003740-a3a1-41fd-9180-e3e43c310706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "535c70f9-053d-42ba-95f7-a5eb5fc114c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 1, Total unique PINs so far: 40250\n",
      "Processed chunk 2, Total unique PINs so far: 81249\n",
      "Processed chunk 3, Total unique PINs so far: 120544\n",
      "Processed chunk 4, Total unique PINs so far: 161068\n",
      "Processed chunk 5, Total unique PINs so far: 201237\n",
      "Processed chunk 6, Total unique PINs so far: 240834\n",
      "Processed chunk 7, Total unique PINs so far: 280808\n",
      "Processed chunk 8, Total unique PINs so far: 320544\n",
      "Processed chunk 9, Total unique PINs so far: 359854\n",
      "Processed chunk 10, Total unique PINs so far: 399145\n",
      "Processed chunk 11, Total unique PINs so far: 438637\n",
      "Processed chunk 12, Total unique PINs so far: 478160\n",
      "Processed chunk 13, Total unique PINs so far: 519617\n",
      "Processed chunk 14, Total unique PINs so far: 558752\n",
      "Processed chunk 15, Total unique PINs so far: 599488\n",
      "Processed chunk 16, Total unique PINs so far: 638892\n",
      "Processed chunk 17, Total unique PINs so far: 680385\n",
      "Processed chunk 18, Total unique PINs so far: 720811\n",
      "Processed chunk 19, Total unique PINs so far: 759800\n",
      "Processed chunk 20, Total unique PINs so far: 799983\n",
      "Processed chunk 21, Total unique PINs so far: 841923\n",
      "Processed chunk 22, Total unique PINs so far: 882800\n",
      "Processed chunk 23, Total unique PINs so far: 922466\n",
      "Processed chunk 24, Total unique PINs so far: 961722\n",
      "Processed chunk 25, Total unique PINs so far: 1001359\n",
      "Processed chunk 26, Total unique PINs so far: 1041556\n",
      "Processed chunk 27, Total unique PINs so far: 1081851\n",
      "Processed chunk 28, Total unique PINs so far: 1121403\n",
      "Processed chunk 29, Total unique PINs so far: 1139209\n",
      "Unique PINs saved to: C:/Users/kaur6/Downloads/Urban Analytics/unique_pins.csv\n",
      "Total unique PINs: 1139209\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define chunk size (adjust based on memory)\n",
    "chunk_size = 1000000  # Reduce if needed\n",
    "\n",
    "# File paths\n",
    "input_file = \"C:/Users/kaur6/Downloads/Urban Analytics/final_cook_county_data_with_same_dtype.csv\"\n",
    "output_file = \"C:/Users/kaur6/Downloads/Urban Analytics/unique_pins.csv\"\n",
    "\n",
    "# Initialize a set to store unique PINs (sets are memory-efficient for uniqueness)\n",
    "unique_pins = set()\n",
    "\n",
    "# Process file in chunks\n",
    "with pd.read_csv(input_file, chunksize=chunk_size, usecols=['pin']) as reader:\n",
    "    for i, chunk in enumerate(reader):\n",
    "        # Remove NaN values and convert PINs to strings (if not already)\n",
    "        chunk = chunk.dropna(subset=['pin'])  \n",
    "        chunk['pin'] = chunk['pin'].astype(str)\n",
    "\n",
    "        # Add unique PINs from this chunk to the set\n",
    "        unique_pins.update(chunk['pin'].unique())\n",
    "\n",
    "        print(f\"Processed chunk {i + 1}, Total unique PINs so far: {len(unique_pins)}\")\n",
    "\n",
    "# Convert set to DataFrame for saving\n",
    "df_unique_pins = pd.DataFrame({'pin': list(unique_pins)})\n",
    "\n",
    "# Save unique PINs to a CSV file\n",
    "df_unique_pins.to_csv(output_file, index=False)\n",
    "print(f\"Unique PINs saved to: {output_file}\")\n",
    "print(f\"Total unique PINs: {len(unique_pins)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13de5d8a-a057-4900-89cd-a998a83b0066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 1\n",
      "Processed chunk 2\n",
      "Processed chunk 3\n",
      "Processed chunk 4\n",
      "Processed chunk 5\n",
      "Processed chunk 6\n",
      "Processed chunk 7\n",
      "Processed chunk 8\n",
      "Processed chunk 9\n",
      "Processed chunk 10\n",
      "Processed chunk 11\n",
      "Processed chunk 12\n",
      "Merged file saved as pin_lat_long.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define chunk size for processing\n",
    "chunk_size = 100000  # Adjust as needed\n",
    "\n",
    "# File paths\n",
    "cook_county_file = \"C:/Users/kaur6/Downloads/Urban Analytics/unique_pins.csv\"\n",
    "parcel_file = \"C:/Users/kaur6/Downloads/Urban Analytics/Parcel_CC.csv\"\n",
    "output_file = \"C:/Users/kaur6/Downloads/Urban Analytics/pin_lat_long.csv\"\n",
    "\n",
    "# Load parcel dataset (ensuring unique PINs)\n",
    "parcel_df = pd.read_csv(parcel_file, usecols=[\"name\", \"latitude\", \"longitude\"], dtype={\"name\": str})\n",
    "parcel_df.drop_duplicates(subset=[\"name\"], inplace=True)  # Ensure unique PINs\n",
    "\n",
    "# Process cook_county dataset in chunks and merge\n",
    "with pd.read_csv(cook_county_file, chunksize=chunk_size, dtype={\"pin\": str}) as reader:\n",
    "    for i, chunk in enumerate(reader):\n",
    "        # Remove duplicate PINs in chunk\n",
    "        chunk.drop_duplicates(subset=[\"pin\"], inplace=True)\n",
    "\n",
    "        # Merge with parcel data to get latitude and longitude\n",
    "        merged_chunk = chunk.merge(parcel_df, left_on=\"pin\", right_on=\"name\", how=\"left\")\n",
    "\n",
    "        # Select only required columns\n",
    "        merged_chunk = merged_chunk[[\"pin\", \"latitude\", \"longitude\"]]\n",
    "\n",
    "        # Remove any duplicate rows after merging\n",
    "        merged_chunk.drop_duplicates(inplace=True)\n",
    "\n",
    "        # Write to file (append after the first chunk)\n",
    "        mode = \"w\" if i == 0 else \"a\"\n",
    "        header = i == 0  # Write header only for the first chunk\n",
    "        merged_chunk.to_csv(output_file, mode=mode, index=False, header=header)\n",
    "\n",
    "        print(f\"Processed chunk {i + 1}\")\n",
    "\n",
    "print(\"Merged file saved as pin_lat_long.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f7322-ed0f-4f18-add0-c8ae77509f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
